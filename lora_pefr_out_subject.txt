(lora-ft) [devops@ansatz lora_ft]$ uv run src/svlearn_lora_ft_subject_chunks/lora_ft.py 
WARNING:root:No configuration file specified. Trying the default location
WARNING:root:Loading configuration from /home/devops/shubeeksh/ai_agents_bootcamp/lora_ft/config.yaml if it exists
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map:  14%|██████████████████▏                                                                                                                 | 6000/43497 [00:13<01:23, 450.48Map:  16%|█████████████████████▏                                                                                                              | 7000/43497 [00:15<01:20, 452.43Map:  18%|████████████████████████▎                                                                                                           | 8000/43497 [00:17<01:18, 451.14Map:  21%|███████████████████████████▎                                                                                                        | 9000/43497 [00:19<01:16, 449.69Map:  23%|██████████████████████████████                                                                                                     | 10000/43497 [00:22<01:14, 449.60Map:  25%|█████████████████████████████████▏                                                                                                 | 11000/43497 [00:24<01:12, 445.90Map:  28%|████████████████████████████████████▏                                                                                              | 12000/43497 [00:26<01:10, 447.18Map:  30%|███████████████████████████████████████▏                                                                                           | 13000/43497 [00:28<01:08, 445.76Map:  32%|██████████████████████████████████████████▏                                                                                        | 14000/43497 [00:31<01:05, 450.74Map:  34%|█████████████████████████████████████████████▏                                                                                     | 15000/43497 [00:33<01:03, 446.30Map:  37%|████████████████████████████████████████████████▏                                                                                  | 16000/43497 [00:35<01:01, 445.67Map:  39%|███████████████████████████████████████████████████▏                                                                               | 17000/43497 [00:37<00:59, 447.81Map:  41%|██████████████████████████████████████████████████████▏                                                                            | 18000/43497 [00:40<00:56, 448.98Map:  44%|█████████████████████████████████████████████████████████▏                                                                         | 19000/43497 [00:42<00:54, 445.84Map:  46%|████████████████████████████████████████████████████████████▏                                                                      | 20000/43497 [00:44<00:52, 445.84Map:  48%|███████████████████████████████████████████████████████████████▏                                                                   | 21000/43497 [00:46<00:49, 450.50Map:  51%|██████████████████████████████████████████████████████████████████▎                                                                | 22000/43497 [00:48<00:47, 454.18Map:  53%|█████████████████████████████████████████████████████████████████████▎                                                             | 23000/43497 [00:51<00:45, 452.40Map:  55%|████████████████████████████████████████████████████████████████████████▎                                                          | 24000/43497 [00:53<00:42, 453.64Map:  57%|███████████████████████████████████████████████████████████████████████████▎                                                       | 25000/43497 [00:55<00:40, 454.26Map:  60%|██████████████████████████████████████████████████████████████████████████████▎                                                    | 26000/43497 [00:57<00:38, 454.54Map:  62%|█████████████████████████████████████████████████████████████████████████████████▎                                                 | 27000/43497 [01:00<00:36, 449.13Map:  64%|████████████████████████████████████████████████████████████████████████████████████▎                                              | 28000/43497 [01:02<00:34, 446.42Map:  67%|███████████████████████████████████████████████████████████████████████████████████████▎                                           | 29000/43497 [01:04<00:32, 447.19Map:  69%|██████████████████████████████████████████████████████████████████████████████████████████▎                                        | 30000/43497 [01:06<00:30, 443.09Map:  71%|█████████████████████████████████████████████████████████████████████████████████████████████▎                                     | 31000/43497 [01:09<00:28, 440.68Map:  74%|████████████████████████████████████████████████████████████████████████████████████████████████▎                                  | 32000/43497 [01:11<00:25, 444.21Map:  76%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                               | 33000/43497 [01:13<00:23, 448.71Map:  78%|██████████████████████████████████████████████████████████████████████████████████████████████████████▍                            | 34000/43497 [01:15<00:20, 452.60Map:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍                         | 35000/43497 [01:17<00:18, 451.10Map:  83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                      | 36000/43497 [01:20<00:16, 452.04Map:  85%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                   | 37000/43497 [01:22<00:14, 450.85Map:  87%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                | 38000/43497 [01:24<00:12, 450.82Map:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍             | 39000/43497 [01:26<00:10, 448.48Map:  92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍          | 40000/43497 [01:29<00:07, 446.30Map:  94%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍       | 41000/43497 [01:31<00:05, 444.30Map:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 42000/43497 [01:33<00:03, 443.05Map:  99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 43000/43497 [01:35<00:01, 441.32Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 43497/43497 [01:37<00:00, 440.12Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 43497/43497 [01:37<00:00, 448.23 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10875/10875 [00:23<00:00, 456.26 examples/s]
trainable params: 740,355 || all params: 67,696,134 || trainable%: 1.0936
/home/devops/shubeeksh/ai_agents_bootcamp/lora_ft/src/svlearn_lora_ft_subject_chunks/lora_ft.py:210: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
Starting training with PEFT LoRA for subject classification...
{'loss': 0.3902, 'grad_norm': 0.41105809807777405, 'learning_rate': 1.877651097217114e-05, 'epoch': 0.18}                                                                      
{'loss': 0.0999, 'grad_norm': 2.118629217147827, 'learning_rate': 1.7550570062522987e-05, 'epoch': 0.37}                                                                       
{'loss': 0.0871, 'grad_norm': 0.11082099378108978, 'learning_rate': 1.6324629152874833e-05, 'epoch': 0.55}                                                                     
{'loss': 0.0799, 'grad_norm': 1.9233944416046143, 'learning_rate': 1.5098688243226678e-05, 'epoch': 0.74}                                                                      
{'loss': 0.0753, 'grad_norm': 0.3070952594280243, 'learning_rate': 1.3872747333578522e-05, 'epoch': 0.92}                                                                      
{'eval_loss': 0.06797757744789124, 'eval_accuracy': 0.9813333333333333, 'eval_precision': 0.9814400317884421, 'eval_recall': 0.9813333333333333, 'eval_f1': 0.9812975316121241, 'eval_runtime': 17.424, 'eval_samples_per_second': 624.14, 'eval_steps_per_second': 19.513, 'epoch': 1.0}                                                                     
{'loss': 0.0673, 'grad_norm': 0.11930210143327713, 'learning_rate': 1.2646806423930366e-05, 'epoch': 1.1}                                                                      
{'loss': 0.0794, 'grad_norm': 1.1423206329345703, 'learning_rate': 1.1420865514282214e-05, 'epoch': 1.29}                                                                      
{'loss': 0.069, 'grad_norm': 5.98876428604126, 'learning_rate': 1.0194924604634058e-05, 'epoch': 1.47}                                                                         
{'loss': 0.068, 'grad_norm': 2.7577247619628906, 'learning_rate': 8.968983694985902e-06, 'epoch': 1.66}                                                                        
{'loss': 0.0745, 'grad_norm': 3.0380563735961914, 'learning_rate': 7.743042785337748e-06, 'epoch': 1.84}                                                                       
{'eval_loss': 0.058661144226789474, 'eval_accuracy': 0.9832643678160919, 'eval_precision': 0.983318163918723, 'eval_recall': 0.9832643678160919, 'eval_f1': 0.9832537494535595, 'eval_runtime': 17.4216, 'eval_samples_per_second': 624.227, 'eval_steps_per_second': 19.516, 'epoch': 2.0}                                                                   
{'loss': 0.0652, 'grad_norm': 1.6453732252120972, 'learning_rate': 6.517101875689592e-06, 'epoch': 2.02}                                                                       
{'loss': 0.0596, 'grad_norm': 0.18223612010478973, 'learning_rate': 5.291160966041437e-06, 'epoch': 2.21}                                                                      
{'loss': 0.0686, 'grad_norm': 0.10527954250574112, 'learning_rate': 4.065220056393282e-06, 'epoch': 2.39}                                                                      
{'loss': 0.0696, 'grad_norm': 0.022027935832738876, 'learning_rate': 2.839279146745127e-06, 'epoch': 2.57}                                                                     
{'loss': 0.0641, 'grad_norm': 0.11246433854103088, 'learning_rate': 1.6133382370969721e-06, 'epoch': 2.76}                                                                     
{'loss': 0.059, 'grad_norm': 0.12797117233276367, 'learning_rate': 3.87397327448817e-07, 'epoch': 2.94}                                                                        
{'eval_loss': 0.05701020732522011, 'eval_accuracy': 0.9837241379310345, 'eval_precision': 0.9837991947188562, 'eval_recall': 0.9837241379310345, 'eval_f1': 0.9837089410691456, 'eval_runtime': 17.4294, 'eval_samples_per_second': 623.947, 'eval_steps_per_second': 19.507, 'epoch': 3.0}                                                                   
{'train_runtime': 541.8869, 'train_samples_per_second': 240.809, 'train_steps_per_second': 15.053, 'train_loss': 0.09178577849601025, 'epoch': 3.0}                            
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8157/8157 [09:01<00:00, 15.05it/s]
Best PEFT model saved to: /home/devops/shubeeksh/lora_ft_op/peft/best_model
Training started at: Fri Sep 26 04:17:54 2025
Training ended at: Fri Sep 26 04:26:56 2025
Total training time: 0h 9m 1s
Loading best PEFT model for final evaluation...
/home/devops/shubeeksh/ai_agents_bootcamp/lora_ft/.venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:196: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
  warnings.warn(
/home/devops/shubeeksh/ai_agents_bootcamp/lora_ft/src/svlearn_lora_ft_subject_chunks/lora_ft.py:245: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer1 = Trainer(
Evaluating final PEFT model...
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 340/340 [00:17<00:00, 19.56it/s]
Final evaluation results:
{'eval_loss': 0.05701020732522011, 'eval_model_preparation_time': 0.0008, 'eval_accuracy': 0.9837241379310345, 'eval_precision': 0.9837991947188562, 'eval_recall': 0.9837241379310345, 'eval_f1': 0.9837089410691456, 'eval_runtime': 17.4377, 'eval_samples_per_second': 623.65, 'eval_steps_per_second': 19.498}

================================================================================
PEFT IMPLEMENTATION COMPLETED SUCCESSFULLY!
================================================================================
Key advantages of using PEFT LoRA over full fine-tuning:
1. Parameter efficiency: ~1-5M trainable vs ~66M trainable parameters
2. Faster training: Significantly reduced training time
3. Lower memory usage: Reduced GPU memory requirements
4. Smaller model size: Only LoRA weights need to be saved
5. Reduced overfitting: Lower risk of catastrophic forgetting
6. Easier deployment: Smaller model files for production
================================================================================